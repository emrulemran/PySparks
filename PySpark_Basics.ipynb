{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5KopE2TZ99ZTWG9FYKliE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emrulemran/PySparks/blob/main/PySpark_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Launch Notebook from Databricks here....\n",
        "https://lnkd.in/gsWKedJ3\n",
        "\n",
        "1.1 Or You install Anaconda distribution from here..\n",
        "https://lnkd.in/gxmiZdCP\n",
        "\n",
        "2. Start Learning Pyspark Basics from here...\n",
        "https://lnkd.in/gfYG2C2Y\n",
        "https://lnkd.in/gQaeSjbH\n",
        "https://lnkd.in/geK6DUnX\n",
        "https://lnkd.in/gMFqckZd\n",
        "\n",
        "3. Learning Done. Then how do you Practise? You can do here..\n",
        "https://lnkd.in/gVMzKGQS\n",
        "Download sample datasets from any Website like Kaggle and throw your hands on it.\n",
        "\n",
        "4. Work on Pyspark projects from here....\n",
        "https://lnkd.in/gRs_EvV5\n",
        "https://lnkd.in/grhFvNq9\n",
        "https://lnkd.in/gYvYkqJZ\n",
        "\n",
        "Resources are short & crispy because learners shouldn't get confused.\n",
        "\n",
        "These are best resources according to my experience. don't miss to check out.\n",
        "\n",
        "Hope this is find \"Useful\"✌\n",
        "\n",
        "do follow Ajay Kadiyala ✅"
      ],
      "metadata": {
        "id": "j89GG2qy25Vw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZKHV2DNgDPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2d3d51-d1fd-4538-9c09-83df1eaa5a71"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install pyarrow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=57adb6e1740fc8621daad2aa7fd9c9aabfeede7ada2f49842c7ad326cc9387d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from pyarrow) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqmCjWa_gmuK",
        "outputId": "8a4b7793-b613-4dfb-f86a-5b049d800666"
      },
      "source": [],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Ram          |Technology|4000  |\n",
            "|Shyam        |Technology|5600  |\n",
            "|Veer         |Technology|5100  |\n",
            "|Renu         |Accounts  |4000  |\n",
            "|Ram          |Technology|4000  |\n",
            "|Vijay        |Accounts  |4300  |\n",
            "|Shivani      |Accounts  |4900  |\n",
            "|Amit         |Sales     |4000  |\n",
            "|Anupam       |Sales     |3000  |\n",
            "|Anas         |Technology|5100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Renu         |Accounts  |4000  |1         |\n",
            "|Vijay        |Accounts  |4300  |2         |\n",
            "|Shivani      |Accounts  |4900  |3         |\n",
            "|Anupam       |Sales     |3000  |1         |\n",
            "|Amit         |Sales     |4000  |2         |\n",
            "|Ram          |Technology|4000  |1         |\n",
            "|Ram          |Technology|4000  |2         |\n",
            "|Veer         |Technology|5100  |3         |\n",
            "|Anas         |Technology|5100  |4         |\n",
            "|Shyam        |Technology|5600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|         Renu|  Accounts|  4000|   1|\n",
            "|        Vijay|  Accounts|  4300|   2|\n",
            "|      Shivani|  Accounts|  4900|   3|\n",
            "|       Anupam|     Sales|  3000|   1|\n",
            "|         Amit|     Sales|  4000|   2|\n",
            "|          Ram|Technology|  4000|   1|\n",
            "|          Ram|Technology|  4000|   1|\n",
            "|         Veer|Technology|  5100|   3|\n",
            "|         Anas|Technology|  5100|   3|\n",
            "|        Shyam|Technology|  5600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD using parallelize()    \n",
        "list = [1, 2, 3, 4]\n",
        "rdd1 = spark.sparkContext.parallelize(list)\n",
        "print(rdd1.collect())\n",
        "print('Items in this RDD: ', rdd1.count())\n",
        "\n",
        "rdd2 = rdd1.repartition(100)\n",
        "\n",
        "print(rdd2.getNumPartitions())\n",
        "\n",
        "\n",
        "rdd3 = rdd2.coalesce(10)\n",
        "\n",
        "print(rdd3.getNumPartitions())\n"
      ],
      "metadata": {
        "id": "zsHLaHcV6IM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc28a85d-5f60-47eb-e477-2b0ac02113dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4]\n",
            "Items in this RDD:  4\n",
            "100\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD from textFile    \n",
        "txtFilePath = '/content/sample_data/textData.txt'\n",
        "txtRDD = spark.sparkContext.textFile(txtFilePath)\n",
        "print(txtRDD.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF8jH_aC_jVq",
        "outputId": "b087b1f9-9f0c-4097-9a26-1c8606d0112b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's try to follow the complete BDD process, which as we said, is an evolution of TDD. We will try to now anchor it on Agile planning and later we will integrate it with Azure DevOps.\", \"- To simplify things, let's work with a simple Class Library project that we will use to simulate buying/selling stock.\", \"- For this let's assume the following user story (tracked in e.g., Azure DevOps):\", '', 'As a StockApp User', 'I want to purchase a given amount of stock at the latest value', 'So that I can increase the value of my portfolio']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD by applying transformation\n",
        "numList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "numRDD = spark.sparkContext.parallelize(numList)\n",
        "evenRDD = numRDD.filter(lambda x : x > 5)\n",
        "print(evenRDD.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a1-Gy5XAMzH",
        "outputId": "4166558b-9fa4-4e5e-9fe1-443436b89c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD Actions:\n",
        "\n",
        "# RDD Action 1 - collect():\n",
        "numSet = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
        "rdd = spark.sparkContext.parallelize(numSet)\n",
        "print('collect() action: ', rdd.collect())\n",
        "\n",
        "# RDD Action 2 - first():\n",
        "print('first() action: ', rdd.first())\n",
        "\n",
        "# RDD Action 3 - count():\n",
        "print('count() action: ', rdd.count())\n",
        "\n",
        "# RDD Action 4 - max():\n",
        "print('max() action: ', rdd.max())\n",
        "\n",
        "# RDD Action 5 - min():\n",
        "print('min() action: ', rdd.min())\n",
        "\n",
        "# RDD Action 6 - take(n):\n",
        "print('take(n) action: ', rdd.take(3))\n",
        "\n",
        "# RDD Action 7 - take(n):\n",
        "rdd.saveAsTextFile('/content/sample_data/output/out.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmVNH6a2ClX-",
        "outputId": "d724ceac-4d9e-4c62-eed4-96bf8280246b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collect() action:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "first() action:  1\n",
            "count() action:  10\n",
            "max() action:  10\n",
            "min() action:  1\n",
            "take(n) action:  [1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/sample_data/country.csv\"\n",
        "df1 = spark.read.option(\"header\", True).option(\"delimiter\", \"\\t\").format(\"csv\").load(path)\n",
        "df1.createOrReplaceTempView(\"CountryTable\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "t4bKLFYr5y7W",
        "outputId": "9336d0ff-8882-4451-9eaf-ad22b6e14692"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3613b2d1df72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/sample_data/country.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CountryTable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"basics\").master('local').getOrCreate()\n",
        "spark\n",
        "from pyspark.sql.window import Window\t\n",
        "\n",
        "from pyspark.sql.functions import rank\t\n",
        "\n",
        "from pyspark.sql.functions import row_number\t\n",
        "\n",
        "Sample_data = [(\"Ram\", \"Technology\", 4000),\t\n",
        "\n",
        "    (\"Shyam\", \"Technology\", 5600),\t\n",
        "\n",
        "    (\"Veer\", \"Technology\", 5100),\t\n",
        "\n",
        "    (\"Renu\", \"Accounts\", 4000),\t\n",
        "\n",
        "    (\"Ram\", \"Technology\", 4000),\t\n",
        "\n",
        "    (\"Vijay\", \"Accounts\", 4300),\t\n",
        "\n",
        "    (\"Shivani\", \"Accounts\", 4900),\t\n",
        "\n",
        "    (\"Amit\", \"Sales\", 4000),\t\n",
        "\n",
        "    (\"Anupam\", \"Sales\", 3000),\t\n",
        "\n",
        "    (\"Anas\", \"Technology\", 5100)\t\n",
        "\n",
        "  ]\t\n",
        "\n",
        "\n",
        "Sample_columns= [\"employee_name\", \"department\", \"salary\"]\t\n",
        "\n",
        "\n",
        "dataframe = spark.createDataFrame(data =Sample_data, schema = Sample_columns)\t\n",
        "\n",
        "dataframe.printSchema()\t\n",
        "\n",
        "dataframe.show(truncate=False)\t\n",
        "\n",
        "\n",
        "# Defining row_number() function\t\n",
        "\n",
        "Window_Spec  = Window.partitionBy(\"department\").orderBy(\"salary\")\t\n",
        "\n",
        "\n",
        "dataframe.withColumn(\"row_number\",row_number().over(Window_Spec)).show(truncate=False)\t\n",
        "\n",
        "\n",
        "# Defining rank() function\t\n",
        "\n",
        "dataframe.withColumn(\"rank\",rank().over(Window_Spec)).show()\t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnMzXlOM1NVR",
        "outputId": "5e6bd534-12d7-4e95-df17-b41a4a503bfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|Ram          |Technology|4000  |\n",
            "|Shyam        |Technology|5600  |\n",
            "|Veer         |Technology|5100  |\n",
            "|Renu         |Accounts  |4000  |\n",
            "|Ram          |Technology|4000  |\n",
            "|Vijay        |Accounts  |4300  |\n",
            "|Shivani      |Accounts  |4900  |\n",
            "|Amit         |Sales     |4000  |\n",
            "|Anupam       |Sales     |3000  |\n",
            "|Anas         |Technology|5100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Renu         |Accounts  |4000  |1         |\n",
            "|Vijay        |Accounts  |4300  |2         |\n",
            "|Shivani      |Accounts  |4900  |3         |\n",
            "|Anupam       |Sales     |3000  |1         |\n",
            "|Amit         |Sales     |4000  |2         |\n",
            "|Ram          |Technology|4000  |1         |\n",
            "|Ram          |Technology|4000  |2         |\n",
            "|Veer         |Technology|5100  |3         |\n",
            "|Anas         |Technology|5100  |4         |\n",
            "|Shyam        |Technology|5600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|         Renu|  Accounts|  4000|   1|\n",
            "|        Vijay|  Accounts|  4300|   2|\n",
            "|      Shivani|  Accounts|  4900|   3|\n",
            "|       Anupam|     Sales|  3000|   1|\n",
            "|         Amit|     Sales|  4000|   2|\n",
            "|          Ram|Technology|  4000|   1|\n",
            "|          Ram|Technology|  4000|   1|\n",
            "|         Veer|Technology|  5100|   3|\n",
            "|         Anas|Technology|  5100|   3|\n",
            "|        Shyam|Technology|  5600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}