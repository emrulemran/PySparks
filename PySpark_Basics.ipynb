{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZMLL8ZCfLnaChtdW1c6q/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emrulemran/PySparks/blob/main/PySpark_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZKHV2DNgDPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0cd45b-c508-4faa-80a4-9c2e47ab2e85"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install pyarrow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=25fb7c71684c4a56e218490135712601a3e4a41556631b4f259a4344d52eef72\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "sqmCjWa_gmuK",
        "outputId": "80b50f1e-ea82-4631-99f9-f0590358366a"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"basics\").master('local').getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f95234fe340>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://aee1f37aa319:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>basics</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD using parallelize()    \n",
        "list = [1, 2, 3, 4]\n",
        "rdd1 = spark.sparkContext.parallelize(list)\n",
        "print(rdd1.collect())\n",
        "print('Items in this RDD: ', rdd1.count())"
      ],
      "metadata": {
        "id": "zsHLaHcV6IM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a515be9-6824-415e-c467-59e462897af8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4]\n",
            "Items in this RDD:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD from textFile    \n",
        "txtFilePath = '/content/sample_data/textData.txt'\n",
        "txtRDD = spark.sparkContext.textFile(txtFilePath)\n",
        "print(txtRDD.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF8jH_aC_jVq",
        "outputId": "b087b1f9-9f0c-4097-9a26-1c8606d0112b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's try to follow the complete BDD process, which as we said, is an evolution of TDD. We will try to now anchor it on Agile planning and later we will integrate it with Azure DevOps.\", \"- To simplify things, let's work with a simple Class Library project that we will use to simulate buying/selling stock.\", \"- For this let's assume the following user story (tracked in e.g., Azure DevOps):\", '', 'As a StockApp User', 'I want to purchase a given amount of stock at the latest value', 'So that I can increase the value of my portfolio']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD by applying transformation\n",
        "numList = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "numRDD = spark.sparkContext.parallelize(numList)\n",
        "evenRDD = numRDD.filter(lambda x : x > 5)\n",
        "print(evenRDD.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a1-Gy5XAMzH",
        "outputId": "4166558b-9fa4-4e5e-9fe1-443436b89c41"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RDD Actions:\n",
        "\n",
        "# RDD Action 1 - collect():\n",
        "numSet = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
        "rdd = spark.sparkContext.parallelize(numSet)\n",
        "print('collect() action: ', rdd.collect())\n",
        "\n",
        "# RDD Action 2 - first():\n",
        "print('first() action: ', rdd.first())\n",
        "\n",
        "# RDD Action 3 - count():\n",
        "print('count() action: ', rdd.count())\n",
        "\n",
        "# RDD Action 4 - max():\n",
        "print('max() action: ', rdd.max())\n",
        "\n",
        "# RDD Action 5 - min():\n",
        "print('min() action: ', rdd.min())\n",
        "\n",
        "# RDD Action 6 - take(n):\n",
        "print('take(n) action: ', rdd.take(3))\n",
        "\n",
        "# RDD Action 7 - take(n):\n",
        "rdd.saveAsTextFile('/content/sample_data/output/op.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmVNH6a2ClX-",
        "outputId": "8aefec94-b501-4bf0-8723-d1f66fc6f4c6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collect() action:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "first() action:  1\n",
            "count() action:  10\n",
            "max() action:  10\n",
            "min() action:  1\n",
            "take(n) action:  [1, 2, 3]\n"
          ]
        }
      ]
    }
  ]
}